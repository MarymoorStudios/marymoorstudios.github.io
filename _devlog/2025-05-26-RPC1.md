---
order: 3
title: "Message Passing and RPC"
date: 2025-04-23 16:19:00 -0700
bg: white
color: black
excerpt: "Message Passing and RPC are foundational mechanisms in any distributed system."
featured: "true"
---
# Message Passing and RPC
--------

## Intro
In the previous post I discussed ordering, concurrency, and interleavings.  In this post I'll delve into some of the
design goals for the RPC package that forms the backbone of all our software built on the [MSC (Marymoor Studios Core
libraries)][MSC].

## Distributed Systems
Traditionally we define a **Distributed System** as a set of program components executing on one or more computers all
of which are connected together by a network.  The components work together to achieve a higher level goal such as
providing a service like a database or a game server.  Most distributed systems are designed to appear logically as a
single system to their users (whether those users are human or other programs).  There is a well-studied field in 
computer science called **[Distributed Computing][distributed-computing]** dedicated to the design, development, and 
understanding of distributed systems.  Distributed computing offers a rich body of learnings and best practices for
building robust distributed systems.

But, how is distributed computing relevant to game design?  Sure, there can be highly scalable game servers, and some
games have multiplayer systems.  Obviously, developing _those_ would benefit from distributed computing expertise.
However, I think game design (or pretty much any modern software) has much more to learn from distributed computing than
just that.  

In my [Ordering Post][devlog-post2] we discussed "Activities" and recognized that games can be made of multiple
concurrent activities.  Concurrent activities can be independent or can close over shared state (as we saw in the
example where two activities shared the same "weapon charge" variable).  Furthermore, we saw that when multiple
activities share state there is a higher degree of interleaving nondeterminism making those programs harder to write and
harder to debug. _But_ if we only favor independent activities (a bunch of concurrent components executing with
completely separate state and never touching each other) then we don't end up with a very interesting game - unless -
they can talk to each other, say through message passing of some kind!

Let's define a distributed system slightly differently:

> **Distributed System**
> 
> A system with multiple independent concurrent activities which communicate with each other by message passing so as
> to coordinate their operation to achieve a higher goal.

With _this_ definition, we can see that all our games are _actually_ distributed systems with many different independent
activities executing concurrently and communicating with each other to implement the game logic.  We can leverage the
learnings from distributed computing to make games that are easier to build, more correct, more robust, and more bug
free!

## Global vs. Local Reasoning
Why should we choose message passing over shared state?  What's wrong with two activities both mutating the same
variable?  Nothing!  In fact, most of our game components _are_ made of multiple activities that all close over the
shared state of that component.  But, as components grow larger and more complicated it becomes more and more difficult
to reason about the correctness of activity interactions over shared state.

Consider the case where the entire game has just _one big global block of state_ and all activities making up the game
are running concurrently and mutating that state as they go.  Then reasoning about the correctness of the program
requires us to look at _all_ of these activities together.  Reasoning about the correctness of any one activity will
_NOT_ be sufficient to know if there is a bug, because some other activity could, during its turn, mutate the global
state in a way that _this_ activity doesn't expect or doesn't want.  This is called **Global Reasoning**.  You have to
think _globally_ about the entire program (or a large portion of it) when deciding if the program logic is correct.  

However, if the game were to divide the state up into smaller pieces, each could be owned by only a single component.
Think of this as a set of islands of computation, which each island has its own state and activities, but the islands
communicate with each other only through message passing.  Then we only need to reason about those activities that have
access to _a single_ piece of state (those on the same island).  We can safely ignore all the rest because they are
independent of the activities we are considering.  This leads to **Local Reasoning** where we only have to look at a
small subset of the program to determine if that part of the program is correct.  In general, local reasoning scales
much better than global reasoning due to inherent physical limitations (human cognitive load, screen size, test
execution latency, etc.).  Maximizing the opportunities for local reasoning while minimizing the places where global
reasoning is required will make our programs easier to build, easier to maintain, and more correct.

When these islands of computation _do_ need to communicate with each other, they will need some mechanism for message
passing.  

## Message Specifications
I strongly prefer a formal and explicit mechanism for message passing (RPC) than informal or ad hoc ones
(key/value, dictionaries, property bags, JSON, etc.).  Formal mechanisms require you to define explicitly which messages
a component can accept and what the shape of those messages will be.  This is usually done through some written
specification. Message specifications are great!  They formalize the information being exchanged.  They clarify the
requirements and assumptions of both the **caller** (sender) and the **callee** (receiver). They form a contract between
the caller and the callee.  All of these things are particularly important when the caller component and the callee
component are being written by different people or different teams or even at different times.  Lastly, the
specification itself becomes a physical _artifact_ in source control that tracks the evolution of these contracts
between components making their review and the avoidance of unintentional breaking changes easier.

## Message Interleaving
I prefer message passing mechanisms that provide some control over _when_ incoming messages are dispatched.  Remember
our discussion from the [Ordering Post][devlog-post2] regarding nondeterminism.  If incoming messages can be dispatched
at any time and in parallel with existing executing activities, then each incoming message will create additional
interleaving nondeterminism.  Ideally, new incoming messages are only dispatched _between_ turn boundaries. This means
that all message handlers will have the **[Top of Turn](/devlog/Glossary#top-of-turn)** property.  This property is
particularly important in prevent reentrancy issues that so frequently occur in games (and other programs) when one
component calls another only for it to immediately call back into the first.  If these calls were regular functions
instead of messages, or if the messages can be dispatched in parallel with existing activities, then the incoming return
facing message will reenter the first component, perhaps _before it has restored its invariants_.  Whether this leads to
corruption depends on the particular interleaving choosen (e.g. timing and rare interleavings).  If locks are used to
protect the state from safety issues, then reentrancy may lead to deadlock.  By deferring dispatches to the next _top of
turn_, reentrancy and deadlock issues are made impossible by construction.

## Location Independence
I prefer message passing mechanisms that provide **Location Independence**.  That is, they abstract from the caller any
way of determining the relative location of the callee.  By "location" here we mean whether the caller and callee are in
the same SIP (the same scheduler), different SIPs in the same process (cross-thread), different processes on the same
machine (cross-process), or different machines (cross-network).

What is the shape of the call model?  How does it represent arguments and return values?  How does it track completion?
How does it represents errors?  Early RPC systems tried to make "remote calls" _look like_ "local calls". This had
several ugly seams.  First, remote calls have significantly different latency characteristics.  Second, remote calls
have significantly different failure modes.  The local call model doesn't provide good extensibility points to represent
these failure modes.  Blocking calls are a poor fit to modern system with asynchronous IO as we discussed previously in
the [Ordering Post][devlog-post2].  Modern RPC systems typically give up on the local call illusion and instead simply
provide an async remote call model that supports remote failure modes.  With either solution, however, it remains
difficult to switch between the local and remote model.  Each has it own style of code.

Ideally, local calls should look and behave identically to remote calls (with the exception of the actual latency of
execution in the absence of real load).  This guarantees that the code for the caller will look no different whether the
call is local or remote.  There MUST be no impact to the programming model, such as how errors are handled, how
completion is tracked or awaited, or how the results are consumed.  This makes it possible to easily change the topology
of concurrently executing activities - even dynamically at runtime!  To move an activity from same SIP to different SIPs
(cross-thread) if the machine happens to have more cores.  To move an activity to a child process to achieve greater
isolation.  To move an activity to another machine to achieve greater scale.  To move an activity behind a load balancer
to achieve fault tolerance.  All of these topology changes can be made with altering the implementation of either caller
or callee.  This provides powerful degrees of freedom for evolving components and for component reuse.

## Scaling and Performance
I prefer message passing mechanism that scale performance proportionally.  How does the cost of sending a message scale
along the spectrum between the same-SIP case on the left and the cross-network case on the right?  How does the latency
scale?  How much overhead is there on the left side of the spectrum (i.e. is it pay-for-play or a high upfront cost)?
How impactful is the programming model on the design of the callee?  Ideally, when the caller and callee are close to
each other, the cost of sending a message is cheap, the latency is small, there are minimal upfront costs, and the
impact to the callee's programming model negligible.  Unfortunately, for most modern messaging systems (e.g. REST,
ASP.NET, gRPC) the upfront costs are high, the latency is not proportional, and the impact to the receiver programming
model is highly asymmetric, requiring complex listeners with networking, security, and resource contention
considerations.

## Testability and Logging
I prefer message passing mechanisms that provide efficient and low cost diagnostic logging.  An important observation
about a system formed from independent activities that communicate _only_ through message passing is: a given sequence
of messages represents the _entire_ interaction with a receiving component.  If the component has a high degree of
determinism, then a particular playthrough of that component can easily be reproduced by appplying the same initial
conditions and a replay of the message sequence.  This is true regardless of the internal states of all other components
(because an independent component takes no direct dependency on the state of any other component).

This means that independent components designed around message passing can easily be tested in isolation.  Unit tests
need not execute or even instantiate any other components.  They need only reproduce the desired message sequences.
Being able to test large parts of the game without having to actual execute the game is a huge aid to correctness.

Ideally logging also supports multiple levels of detail that can optionally capture either only the message sequence
(without payloads to preserve privacy) or the fully message content for easy debuggability and replay.  Logging should
be cheap enough that the most basic level can be turned on in production.  Logging should be near real-time enough such
that it can be used interactively during development and testing.

## Promise RPC
So to summarize, we were looking for a message passing system that:

* Has formal explicit specifications, but is super easy for C# developers to use.  One that doesn't requiring learning a
  new specification language or using usual 3rd party tools.
* Works with our existing scheduler.  Does not introduce new interleaving nondeterminism, reentrancy, or locking issues.
* Delivers Location Independence such that we can write components only once for _both_ local and remote use.
* Is cheap and easy enough to use between components in the same SIP that we wouldn't hesitate to use it everywhere, but
  one that still scales up to handle cross-thread, cross-process, and cross-network scenarios without introducing a lot
  of complexity.
* Is simple to use in unit testing for component isolation and validation.
* Has an efficient logging system with near real-time interactivity.

From this requirements list was born the [Promise RPC library](
https://www.nuget.org/packages/MarymoorStudios.Core.Rpc/) as part of the larger [MSC (Marymoor Studios Core)][MSC].  

Promise RPC provides formal explicit specifications through the definition of C# interfaces with the
[\[Eventual\]](https://marymoorstudios.com/_docfx/api/MarymoorStudios.Core.Rpc.EventualAttribute.html) attribute.  The
specification language is just normal idiomatic C# supporting almost all familiar syntax features including method
overloads, optional arguments, nullable reference types, parameter attribute tunneling, doc-comments, and even generic
interfaces. Flexible parameter modeling support (provided by the [MSC][MSC] Serialization package which I'll talk more
in a future post) allows parameters of any primitive type, user-defined data contract types, sequences, byte streams,
and capabilities (which I'll also talk more about in the next post).

The Promise RPC system includes a Roslyn-integrated code [generator](
https://www.nuget.org/packages/MarymoorStudios.Core.Generators/) that generates proxy and stub types automatically
during the execution of the normal C# compiler toolchain.  By including a reference to the generator package, Visual
Studios generates proxy/stub code on-demand as you type, provides full completion, and doc-comment-based intellisense.
Eventual interfaces can be both defined and implemented in the same assembly due to this unique compiler integration.

All eventual methods return Promise-based asynchronous values that isolate both return value tracking and failure
discovery providing true Location Independence.  This also tightly integrates callers with the Promise Scheduler
(discussed in the [previous post][devlog-post2]) and supports call concurrency, completion tracking,
[awaiting][await-keyword], and first-class Promise-based composition.  The generated stubs provide abstract classes that
can easily be derived from and support [async][async-keyword] method implementations for message dispatch.  The RPC
dispatcher is also tightly integrated with the Promise Scheduler and dispatches incoming messages as top-level
activities, like any other Promise-based activity, providing the same top-of-turn, single-threaded, cooperative,
semantics with guaranteed freedom from reentrancy and locking issues.  

Behind the scenes, the runtime provides automatic routing and guaranteed at most once message delivery.  Local calls
(same SIP) are _very cheap_, incurring only two allocations (a resolver for the result and a message class).
Cross-thread and cross-process dispatches proportionally add serialization and shared memory queuing. Networking
overhead is _only_ incured by messages that actually go remote.  This cost profile makes it easy to use RPC everywhere
in our designs for component isolation, and only worry about topology configuration in a few small places in the program
where the SIPs and channels are orchestrated.  Once the topology is established, none of the components have runtime
visibility into that topology and so cannot take a dependency on it.  

As an example of how all these pieces come together to make writing games easier for us, consider the implementation of
three multiplayer modes: local coop, LAN hosted, and game server.  They are all implemented using the exact same `IGame`
interface and game component implementation. The key differences lie in _where_ that component is instantiated. In local
coop mode the game component is instantiated within the same SIP as the display which the players interact with.  The
display is given only an `IGame` proxy to the game instance.  In the LAN hosted mode, the game component is instantiated
in another SIP (running on another core in the same process and along with the networking package which accepts
connections from the remote players).  The display handles only the local player, but is still given only an `IGame`
proxy to the game instance, calls on which _now_ lead to cross-thread marshalling.  The display component does not know
the difference. In the game server mode, the game component is instantiated on another machine (the game server), but
the display is still given only an `IGame` proxy which _now_ leads to cross-network communication.  The display
component does not know the difference.  We leverage the Promise RPC system to write one implementation of the both
display and game components.  We test these implementations independently of each other and the actual game topology at
runtime.  We leverage location independence to dynamically choose a topology at runtime that meets the needs of the
current game mode.

## Conclusion
Use the RPC...

## Feedback
Write us with [feedback][feedback].

## See Also
* [All Posts][all-posts]
* [Glossary][glossary]
* [MSC (Marymoor Studios Core libraries)][MSC]

[MSC]: https://github.com/MarymoorStudios/Core
[all-posts]: /devlog.html
[devlog-post2]: /_devlog/2025-04-23-Ordering.md
[feedback]: mailto:feedback@marymoorstudios.com
[glossary]: /devlog/Glossary
[async-keyword]: https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/async
[await-keyword]: https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/operators/await
[distributed-computing]: https://en.wikipedia.org/wiki/Distributed_computing