---
order: 6
title: "RPC Part 4: Sequences and Streams"
date: 2025-08-26 12:01:00 -0700
bg: white
color: black
excerpt: "Sequences and Streams allow streaming with flow-control."
featured: "true"
---
# RPC Part 4: Sequences and Streams
--------

## Intro
In the previous post we learned about capabilities and capability exchange.  We examined how capablities provide
elegant, powerful, and highly efficient mechanisms for interacting with remote distributed objects and their state.
Finally, we touched on some of the ways in which capabilities can be used to build secure systems.  In this post we will
look at streaming which provides a model for dealing with data whose length is either very large or even infinite.

## Motivating Example
Consider a multiplayer game architecture from the highest level.  Abstractly, it might consist of a game server and some
number of game clients.  The game server orchestrates the game.  Each of the clients collects input from one of the
players and passes it to the game server.  The game server combines all of the inputs, updates the active state of the
game, and communicates game state changes to the clients.  Finally, each client updates its display to reflect the
current state of the game.  This process continues in a loop indefinitely until the game server (according to the rules
of the game) determines that the game is over.

Let's try to define an eventual interface to manage the communication between the clients and the game server.  A simple
design might look like:

```csharp
[Eventual]
interface IGame
{
  Promise<GameEvent> GetNext();
}
```

Assuming `s` is a `GameProxy`, the main game loop might look like:

```csharp
while (true) {
  switch (await s.GetNext()) {
    // Handle different types of events.
    ...
    case EndGame:
      return;
  }
}
```

This design is very simple, but it works.  Each client will receive a sequence of events by repeatedly calling
`GetNext()`. Each client will handle that event, and then call the server again for the next event.  To maintain the
proper ordering of event handling the client can easily control when the next event is received by controlling when it
calls `GetNext()`.  There are three main drawbacks to this design:

1. Unnecessary round-trips.  
   Each call to `GetNext()` requires a full network round-trip.  The client must first send a message to the server, and
   then the server must respond.  This increases the network latency between events and doesn't allow for any
   pipelining.  This will result in the game being less responsive and less scalable.

2. Additional allocations.  
   Each call requires individual allocations for at least the response promise (and possibly for the `GetNext()` message
   as well, depending the conditions).  Furthermore, there is no batching of events, even if they occur close together.
   Although of these allocations are small, if the number of events is very large this can create additional GC pressure
   resulting in the game also being less responsive and less scalable.

3. The server-side ordering is implicit.  
   The `GetNext()` method doesn't by itself indicate _which_ event should be next.  Different clients may process the
   events at different speeds (due to both networking characteristics and hardware considerations).  The server MUST
   keep track of which events have been sent to each client so that it can return the correct _next_ message for the
   calling client.
   
## Push Example
What if we invert the call flow?  What if we instead define the game server interface to take a capability from the
client.  Then the game server can call methods on that capability any time it wants.:

```csharp
[Eventual]
interface IGame
{
  Promise SetEventCallback(ClientProxy callback);
}
[Eventual]
interface IClient
{
  Promise HandleEvent(GameEvent e);
}
```

Now, instead of having a game loop, the client would set its callback capability and then just wait for the game to be
over, relying on the implementation of the callback to handle all of the game events:

```csharp
ClientHandler handler = new();
await s.SetEventCallback(new ClientProxy(handler));
await handler.Done;

...

sealed class ClientHandler : AClientServer
{
    Promise Done => new(m_done);  // Resolved when the game is over.

    public Promise HandleEvent(GameEvent e) {
      switch (await s.GetNext()) {
        // Handle different types of events.
        ...
        case EndGame:
          m_done.Resolve();  // game is over.
          return;
      }
    }
}
```

This addresses problems (1) and (3):

* Half the network latency.  
  The server can now push events whenever they occur directly to the client.  The client will receive them after only
  half the previous network latency (the path from server to client, without first the client to server needed to send
  the `GetNext()` message above).  That is, the average latency is 1/2 the network RTT.

* Explicit server-side ordering  
  The server doesn't need to wait for the client's response before making another call to `HandleEvent()`. Instead, the
  server can pipeline any number of calls immediately and rely on `Instance Order` to dispatch them at the client in the
  proper order.  The order the server makes the calls _explicitly_ defines the order the game events occurred in.

That seems like an improvement, however, we have now introduced *four* new problems:

4. Loss of client-side ordering.  
   The client will now dispatch the next event _as soon as_ the previous event yields (e.g. _awaits_).  Dispatch Order
   _only_ guarantees strong ordering of the _first turn_ of an activity.  If handling an event takes multiple turns
   (e.g. plays an input-blocking animation), then the next event will dispatch _before_ the previous event is finished.
   Just _awaiting_ within the event handler is no longer sufficient to properly order event processing.  Instead the
   client implementation would need to do something more sophisticated to defer processing of the next event from the
   server.

5. No backpressure.  
   The client has no control over how many events the server can send it at one time.  If the server keeps pushing
   events at a rate faster than the client can handle the events will start to queue up either in the dispatcher or in
   the event handler implementation.  This will eventually lead the client to exhaust its own memory and crash - unless
   it can provide backpressure to the server to tell it to stop or slow down.  With the current design the server has no
   way to know that the client is not keeping up.

6. No way for the client to gracefully exit early.  
   If a player leaves the game early (before the game is finished) there is no graceful way to tell the server to stop
   pushing messages.  We'd need to add another method to `IGame` that would tell the server to stop or to remove the
   callback.

7. No way for the client to detect when the server has crashed.  
   If the game server were to crash, or a network partition were to occur, there is no way for the client to detect it.
   The client is waiting for the server to make calls on its interface.  If the session with the game server were to
   disconnect then the server would simply stop making calls, but the client wouldn't know when to stop waiting.  Since
   the server was disconnected abruptly, no `EndGame` event would ever be sent.

The proper solution to all of these problems is a streaming **Sequence**.

## Sequence&lt;T&gt;
A streaming **Sequence** is actually defined by *two* special, system-defined, capabilities that work together to create
a single abstraction:

* **Consumer: `Sequence<T>`**  
  A capability that gives the holder the right to _read_ a sequence of zero or more (possibly infinite) items (of type
  `T`).

* **Producer: `Sequence<T>.Writer`**  
  A capability that gives the holder the right to _write_ zero or more items (of type `T`) into a sequence.

The reader side of a **Sequence** provides the consumer of the sequence with four fundamental actions:

```csharp
interface ISequence<T>
{
  Promise<T> Read();
  void Cancel(Exception? error = null);
  int Capacity { get; set; }
  int BatchSize { get; set; }
}
```

The ability to read the next item in the sequence.  The ability to gracefully terminate the sequence from the consumer
side and promptly release all client-side resources (optionally with a diagnostic error for the server's use).  The
ability to set the parameters of flow control modelling including its overall in-flight window capacity as well as
limits on automatic batching.

Similarly, the writer side of a **Sequence** provides the producer with four actions:

```csharp
interface ISequenceWriter<T>
{
  void Write(T item);
  Promise Flush();
  Promise Close(Exception? error = null);
  void Abort();
}
```

The ability to write the next item into the sequence.  The ability to observe the current state of backpressure (via
`Flush()`) and to optionally _await_ until backpressure has eased.  The ability to end the sequence gracefully or to
terminate the sequence with a diagnostic error (for the client's information).  And finally, the ability to abruptly
abort the sequence and promptly release all server-side resources.

When a new **Sequence** is created, the two capabilities are returned as a pair.  The creator can then distribute either
the read capability and/or the write capability as it sees fit.

A message passing system that support Capability Exchange (as we discussed in the [previous post][devlog-post5]) may
also support the exchange of either a **Sequence** read capability or write capability (or both).

## Sequence In Action
If our message passing system supports streaming **Sequence** then we can redefine our game server interface to use it:

```csharp
[Eventual]
interface IGame
{
  Sequence<GameEvent> GetEvents();
}
```

Assuming `s` is a `GameProxy`, the main game loop might, once again, look like:

```csharp
foreach (GameEvent e in s.GetEvents()) {
  switch (e) {
    // Handle different types of events.
    ...
    case EndGame:
      return;
  }
}
```

This code is suspiciously similar to the code we started with, however, despite is apparent simplicity, this code has
_none_ of the problems we encountered earlier:

1. Unnecessary round-trips.  
   Though each iteration of the `foreach` loop calls the sequence `Read()` method, this call is NOT a normal eventual
   RPC.  The underlying message passing system is managing an asynchronous stream of items with buffering and flow
   control.  A call to `Read()` is handled promptly (i.e. does NOT need to yield during _await_) if an item already
   exists in the client-side buffer.  Additionally, the server writes items directly into the producer capability and
   the message passing system is responsible for handling buffering, nagling, and coalescing to produce efficient
   batching for asynchronous background transmission that overlaps with both the server and client side processing.
   Like the [Push Example](#push-example) above, the average latency is 1/2 the network RTT.

2. Additional allocations.  
   There are significantly fewer underlying network RPCs in the sequence model.  Since the message passing system is
   providing automatic buffering, nagling, and coalescing, multiple events that occur close together will be combined
   into a single batch for more efficient network transmission.  Additionally, there are no allocations when reads are
   resolved promptly from the client-side buffer.

3. The server-side ordering is implicit.  
   The server writes directly into the writer capability.  The order of the writes explicitly defines the order of the
   items that will be read from the sequence by the client.  The message passing system is responsible for keeping track
   of order as well as the reader's position within the sequence.

4. Loss of client-side ordering.  
   The client-side main loop doesn't extract the next item from the sequence until it starts the next iteration of the
   `foreach` loop.  If the client _awaits_ in the middle of the loop then no items will be extracted or dispatched.  So,
   _awaiting_ is once again sufficient to maintain client-side ordering even when the event processing activity takes
   multiple turns to complete (e.g. an input-blocking animation).

5. No backpressure.  
   The client has full control over both the overall size of the flow control window and its internal batching.  This
   gives the client direct control over how much memory the client is will to commit to event processing.  The message
   passing system's implementation of flow control is responsible for guaranteeing that the server never exceed these
   limits. Additionally, the server can observe client backpressure (communicated automatically in the background by the
   message passing system in the upstream direction client to server).  In response to observed backpressure, the server
   can pause production, coalesce items, or drop (unnecessary or redundant) events to reduce traffic until the client
   catches up.

6. No way for the client to gracefully exit early.  
   The client can gracefully end the sequence by calling `Cancel()` (or by exiting the `foreach` loop with a `break` or
   an exception).  The server can observe the client's sequence cancellation during its next call to `Write()` or
   `Flush()` and then perform its own clean up.

7. No way for the client to detect when the server has crashed.  
   If the game server were to crash, the client will observe the sequence termination in the next call to `Read()`.  The client can then perform its own clean up or recovery.


## Example
Capabilities and capability-based security play a key role in pretty much all of our designs.  But to illustrate how
capabilities make it easier to write games, I'd like to revisit the game lobby example from the [previous
post][devlog-post4].  In that post we were concerned with message ordering as it related to game settings and the
settings' LSN.  Here we will see how capabilities play a important role in securing our multiplayer systems.  The game
lobby defines the following three interfaces (abridged here for illustrative purposes):

```csharp
[Eventual]
interface IGameLauncher
{
  JoinedPlayerProxy TryJoin(PlayerToken slot, ClientLauncherProxy client);
  
  // We talked about this method in the previous post.
  Promise TryLaunch(ulong settingsLsn);
}
[Eventual]
interface IJoinedPlayer
{
  Promise Leave();
}
[Eventual]
interface IClientLauncher
{
  Promise Launch(PlayerProxy player);
}
```

The first two interfaces are implemented by the game server while the last is implemented by the game client.  

When connecting to a game server the client is given a root capability that provides methods for enumerating the
available unlaunched games or creating a new unlaunched game.  When a game with open slots is selected, a capability to
an object implementing the `IGameLauncher` interface is returned.  A client can attempt to join an open game slot by
calling the `TryJoin` method and passing: (1) a `PlayerToken` which identifies the desired slot to claim, and (2) a
capability to an object which implements the `IClientLauncher` interface.  If the join attempt is successful, the server
returns a valid capability to an object implementing the `IJoinedPlayer` interface.  Only clients that hold a valid
`JoinedPlayerProxy` have actually successfully claimed a slot.  In this way, race conditions to join the same slot are
resolved.  

The `JoinedPlayerProxy` capability itself only authorizes the player to leave the game slot (by calling the `Leave`
method, or by discarding the capability).  But, if they hold the `JoinedPlayerProxy` capability until the game is
launched then the server calls the `Launch` method on the `ClientLauncherProxy` capability that the client provided
during the join attempt.  The server passes the `PlayerProxy` capability to the `Launch` method which authorizes the
player to play that specific game as that specific player.  All subsequent method calls on the `PlayerProxy` imply that
those are the actions of _that_ player for _that_ game.  No other security checks are needed on subsequent method
dispatches because _only_ those players that successfully joined a slot were given a `PlayerProxy`, and each
`PlayerProxy` uniquely refers to a particular game slot.

## Conclusion
In this post we talked about capabilities, capability exchange, and how both can be used to build secure, efficient, and
easy to use systems.  We gave some examples of how [Promise RPC library's](
https://www.nuget.org/packages/MarymoorStudios.Core.Rpc/) implementation of capabilities can be used to implement common
game constructs.

This is Part 3 of our look at the [MSC][MSC] RPC system.  In Part 4 and beyond we'll look at streaming with sequences
and bytes, and finally we'll see how channel lifetime relates to aborts and cancellation. Until next time, code on!

## Previous
Read the [previous post][devlog-post5] in this series.

## Feedback
Write us with [feedback][feedback].

## See Also
* [All Posts][all-posts]
* [Glossary][glossary]
* [MSC (Marymoor Studios Core libraries)][MSC]

[MSC]: https://github.com/MarymoorStudios/Core
[all-posts]: /devlog.html
[devlog-post3]: /devlog/2025-05-29-RPC1
[devlog-post4]: /devlog/2025-06-30-RPC2
[devlog-post5]: /devlog/2025-07-29-RPC3
[feedback]: mailto:feedback@marymoorstudios.com
[glossary]: /devlog/Glossary
[distributed-computing]: https://en.wikipedia.org/wiki/Distributed_computing
[capability-based_security]: https://en.wikipedia.org/wiki/Capability-based_security
[oop]: https://en.wikipedia.org/wiki/Object-oriented_programming
